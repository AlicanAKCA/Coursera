{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 2 : Convolutional Neural Networks in TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9hRbbsN-aAhr",
        "OsvzJlfUaJCU",
        "-9ckM7Qapdwk",
        "H2x7RtkFpmoG",
        "ycvGz4acmRUG",
        "kLMLAqMBBm00",
        "Fw5xqtZ38o2c"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PffZzyIIlpry"
      },
      "source": [
        "# Course 2 : Convolutional Neural Networks in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hRbbsN-aAhr"
      },
      "source": [
        "## Week - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsvzJlfUaJCU"
      },
      "source": [
        "### Exercise_1_Cats_vs_Dogs_Question-FINAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-wxhEPcaP__"
      },
      "source": [
        "# ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n",
        "# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n",
        "# ATTENTION: Please use the provided epoch values when training.\n",
        "\n",
        "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
        "# This will require you doing a lot of data preprocessing because\n",
        "# the dataset isn't split into training and validation for you\n",
        "# This code block has all the required inputs\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "from os import getcwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzckBacaR-5"
      },
      "source": [
        "path_cats_and_dogs = f\"{getcwd()}/../tmp2/cats-and-dogs.zip\"\n",
        "shutil.rmtree('/tmp')\n",
        "\n",
        "local_zip = path_cats_and_dogs\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPN8F-raaUQo"
      },
      "source": [
        "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Expected Output:\n",
        "# 1500\n",
        "# 1500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQZbFlY-aVep"
      },
      "source": [
        "# Use os.mkdir to create your directories\n",
        "# You will need a directory for cats-v-dogs, and subdirectories for training\n",
        "# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n",
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdTflZcHaXRt"
      },
      "source": [
        "# Write a python function called split_data which takes\n",
        "# a SOURCE directory containing the files\n",
        "# a TRAINING directory that a portion of the files will be copied to\n",
        "# a TESTING directory that a portion of the files will be copie to\n",
        "# a SPLIT SIZE to determine the portion\n",
        "# The files should also be randomized, so that the training set is a random\n",
        "# X% of the files, and the test set is the remaining files\n",
        "# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n",
        "# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n",
        "# and 10% of the images will be copied to the TESTING dir\n",
        "# Also -- All images should be checked, and if they have a zero file length,\n",
        "# they will not be copied over\n",
        "#\n",
        "# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n",
        "# os.path.getsize(PATH) gives you the size of the file\n",
        "# copyfile(source, destination) copies a file from source to destination\n",
        "# random.sample(list, len(list)) shuffles a list\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    filtered_source = []\n",
        "    \n",
        "    source = os.listdir(SOURCE)\n",
        "    for filename in source:\n",
        "        src = SOURCE + filename\n",
        "        if (os.path.getsize(src) > 0):\n",
        "            filtered_source.append(filename)\n",
        "    \n",
        "    size = len(filtered_source)\n",
        "    \n",
        "    training_size = int(size * SPLIT_SIZE)\n",
        "    test_size = size - training_size\n",
        "    shuffled = random.sample(filtered_source, size)\n",
        "    \n",
        "    training_set = shuffled[0:training_size]\n",
        "    for filename in training_set:\n",
        "        copyfile(SOURCE + filename, TRAINING + filename)\n",
        "    \n",
        "    test_set = shuffled[-test_size:]\n",
        "    for filename in test_set:\n",
        "        copyfile(SOURCE + filename, TESTING + filename)\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l07agq6saZAt"
      },
      "source": [
        "print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# Expected output:\n",
        "# 1350\n",
        "# 1350\n",
        "# 150\n",
        "# 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUG39iR2aa15"
      },
      "source": [
        "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "# USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(), \n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "])\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q0_7_SpacF5"
      },
      "source": [
        "TRAINING_DIR = '/tmp/cats-v-dogs/training/'\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# TRAIN GENERATOR.\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = '/tmp/cats-v-dogs/testing/'\n",
        "validation_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# VALIDATION GENERATOR.\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = 'binary',\n",
        "                                                         target_size = (150, 150))\n",
        "\n",
        "\n",
        "\n",
        "# Expected Output:\n",
        "# Found 2700 images belonging to 2 classes.\n",
        "# Found 300 images belonging to 2 classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9Cvj3oaeW-"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=2,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L_K1eoKafnx"
      },
      "source": [
        "# PLOT LOSS AND ACCURACY\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "\n",
        "\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "# Desired output. Charts with training and validation metrics. No crash :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfhgylJ0al7R"
      },
      "source": [
        "### Optional Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuQaAHK3apKo"
      },
      "source": [
        "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
        "# This will require you doing a lot of data preprocessing because\n",
        "# the dataset isn't split into training and validation for you\n",
        "# This code block has all the required inputs\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdL_Nyxeb9yV",
        "outputId": "04323e59-e1a0-4400-b868-3f85792aff75"
      },
      "source": [
        "# This code block downloads the full Cats-v-Dogs dataset and stores it as \n",
        "# cats-and-dogs.zip. It then unzips it to /tmp\n",
        "# which will create a tmp/PetImages directory containing subdirectories\n",
        "# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n",
        "# If the URL doesn't work, \n",
        "# .   visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-07 10:14:26--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 104.73.0.105, 2600:1406:5800:28f::e59, 2600:1406:5800:2b8::e59\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|104.73.0.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824894548 (787M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
            "\n",
            "/tmp/cats-and-dogs. 100%[===================>] 786.68M  72.9MB/s    in 11s     \n",
            "\n",
            "2021-07-07 10:14:37 (71.4 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEDrG98_cBov",
        "outputId": "fe550506-b820-43a3-af3e-3f677827fae3"
      },
      "source": [
        "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Expected Output:\n",
        "# 12501\n",
        "# 12501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12501\n",
            "12501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVghsv18cC-c"
      },
      "source": [
        "# Use os.mkdir to create your directories\n",
        "# You will need a directory for cats-v-dogs, and subdirectories for training\n",
        "# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n",
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPgSgNC8cF3w"
      },
      "source": [
        "# Write a python function called split_data which takes\n",
        "# a SOURCE directory containing the files\n",
        "# a TRAINING directory that a portion of the files will be copied to\n",
        "# a TESTING directory that a portion of the files will be copie to\n",
        "# a SPLIT SIZE to determine the portion\n",
        "# The files should also be randomized, so that the training set is a random\n",
        "# X% of the files, and the test set is the remaining files\n",
        "# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n",
        "# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n",
        "# and 10% of the images will be copied to the TESTING dir\n",
        "# Also -- All images should be checked, and if they have a zero file length,\n",
        "# they will not be copied over\n",
        "#\n",
        "# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n",
        "# os.path.getsize(PATH) gives you the size of the file\n",
        "# copyfile(source, destination) copies a file from source to destination\n",
        "# random.sample(list, len(list)) shuffles a list\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    filtered_source = []\n",
        "    \n",
        "    source = os.listdir(SOURCE)\n",
        "    for filename in source:\n",
        "        src = SOURCE + filename\n",
        "        if (os.path.getsize(src) > 0):\n",
        "            filtered_source.append(filename)\n",
        "    \n",
        "    size = len(filtered_source)\n",
        "    \n",
        "    training_size = int(size * SPLIT_SIZE)\n",
        "    test_size = size - training_size\n",
        "    shuffled = random.sample(filtered_source, size)\n",
        "    \n",
        "    training_set = shuffled[0:training_size]\n",
        "    for filename in training_set:\n",
        "        copyfile(SOURCE + filename, TRAINING + filename)\n",
        "    \n",
        "    test_set = shuffled[-test_size:]\n",
        "    for filename in test_set:\n",
        "        copyfile(SOURCE + filename, TESTING + filename)\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Expected output\n",
        "# 666.jpg is zero length, so ignoring\n",
        "# 11702.jpg is zero length, so ignoring"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvrteX9UcHzJ",
        "outputId": "5916a641-4af0-4c0c-a32b-acb7ceed08f6"
      },
      "source": [
        "print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# Expected output:\n",
        "# 11250\n",
        "# 11250\n",
        "# 1250\n",
        "# 1250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12370\n",
            "12375\n",
            "2370\n",
            "2375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhkVc5uIcJpc"
      },
      "source": [
        "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "# USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(), \n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "])\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmFIYhYzcMU1",
        "outputId": "dc1afc6d-0cb0-4a05-bc94-bca280df6620"
      },
      "source": [
        "TRAINING_DIR = '/tmp/cats-v-dogs/training/'\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# TRAIN GENERATOR.\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = '/tmp/cats-v-dogs/testing/'\n",
        "validation_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# VALIDATION GENERATOR.\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = 'binary',\n",
        "                                                         target_size = (150, 150))\n",
        "\n",
        "\n",
        "\n",
        "# Expected Output:\n",
        "# Found 2700 images belonging to 2 classes.\n",
        "# Found 300 images belonging to 2 classes."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 24744 images belonging to 2 classes.\n",
            "Found 4744 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqhP_BmOdPre",
        "outputId": "74ce38d3-3aa5-43a0-f60b-d564886aea91"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=2,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1238/1238 [==============================] - 95s 74ms/step - loss: 0.5732 - acc: 0.7019 - val_loss: 0.4773 - val_acc: 0.7639\n",
            "Epoch 2/2\n",
            "1238/1238 [==============================] - 91s 73ms/step - loss: 0.4541 - acc: 0.7914 - val_loss: 0.4142 - val_acc: 0.8122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "vlydu6-wczm6",
        "outputId": "33e21154-8a67-4348-918f-06ecc9d78cac"
      },
      "source": [
        "# PLOT LOSS AND ACCURACY\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "\n",
        "\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "# Desired output. Charts with training and validation metrics. No crash :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and validation loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcYAAAEICAYAAADFgFTtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaxElEQVR4nO3debSfVX3v8fcHkpCkIUEIigMQFBHFVotxgFsVh14prVNrW3GgOJR79d5qe1vv7b12oNa2tlVxuWqdWsShWsQqZUmlXVWRYlUMYrRYbVGgqFgBMRACCMn3/vHsIzsnZ/id5Aw5Oe/XWs86z+8Z937O8Dl7P/v3e1JVSJKkwX4LXQBJkvYmBqMkSR2DUZKkjsEoSVLHYJQkqWMwSpLUMRilaST5WJJfmu1tF1KSa5I8dQ6OW0mObvNvS/Lbo2y7G+d5fpJ/2N1ySlOJ72PUvijJ1u7lauBOYHt7/d+q6q/mv1R7jyTXAC+tqn+c5eMW8OCqumq2tk2yAbgaWF5Vd89GOaWpLFvoAkhzoarWjM1PFQJJlvnHVnsLfx73DnalaklJclKSbyb5P0m+A7wryb2SfDTJDUlubvMP6Pa5OMlL2/zpSS5N8vq27dVJfmo3tz0qySVJbk3yj0nekuR9k5R7lDL+fpJPt+P9Q5L13foXJrk2yU1JXj3F9Xlsku8k2b9b9uwkX2rzj0nymSTfT3J9kj9LsmKSY52T5LXd61e1fb6d5MXjtv3pJFckuSXJdUnO7FZf0r5+P8nWJCeMXdtu/xOTfD7Jlvb1xFGvzQyv88FJ3tXqcHOS87t1z0zyxVaHryc5uS3fqds6yZlj3+ckG1qX8kuS/Afwibb8vPZ92NJ+Ro7r9l+V5A3t+7ml/YytSnJhkl8ZV58vJXn2RHXV5AxGLUWHAQcDRwJnMPwevKu9PgK4HfizKfZ/LPA1YD3wJ8BfJslubPt+4DLgEOBM4IVTnHOUMj4PeBFwb2AF8BsASR4GvLUd/37tfA9gAlX1OeA24Mnjjvv+Nr8d+LVWnxOApwAvn6LctDKc3Mrzk8CDgfH3N28DTgMOAn4aeFmSZ7V1T2hfD6qqNVX1mXHHPhi4EHhzq9sbgQuTHDKuDrtcmwlMd53fy9A1f1w71lmtDI8B3gO8qtXhCcA1k12PCTwReCjwtPb6YwzX6d7AF4C+6//1wKOAExl+jv83sAN4N/CCsY2SPAK4P8O10UxUlZPTPj0x/IF6aps/CfgBsHKK7R8J3Ny9vpihKxbgdOCqbt1qoIDDZrItwx/du4HV3fr3Ae8bsU4TlfG3utcvBy5q878D/HW37kfaNXjqJMd+LXB2mz+QIbSOnGTbXwU+0r0u4Og2fw7w2jZ/NvC6brtj+m0nOO6bgLPa/Ia27bJu/enApW3+hcBl4/b/DHD6dNdmJtcZuC9DAN1rgu3ePlbeqX7+2uszx77PXd0eOEUZDmrbrGMI7tuBR0yw3UrgZob7tjAE6J/P9+/bvjDZYtRSdENV3TH2IsnqJG9vXVO3MHTdHdR3J47znbGZqtrWZtfMcNv7Ad/rlgFcN1mBRyzjd7r5bV2Z7tcfu6puA26a7FwMrcOfTXIA8LPAF6rq2laOY1r34ndaOf6QofU4nZ3KAFw7rn6PTfLJ1oW5BfjvIx537NjXjlt2LUNracxk12Yn01znwxm+ZzdPsOvhwNdHLO9Efnhtkuyf5HWtO/YW7ml5rm/TyonO1X6mzwVekGQ/4FSGFq5myGDUUjR+KPavAw8BHltVa7mn626y7tHZcD1wcJLV3bLDp9h+T8p4fX/sds5DJtu4qr7CECw/xc7dqDB0yX6VoVWyFvh/u1MGhhZz7/3ABcDhVbUOeFt33OmGzn+boeuzdwTwrRHKNd5U1/k6hu/ZQRPsdx3woEmOeRtDb8GYwybYpq/j84BnMnQ3r2NoVY6V4UbgjinO9W7g+Qxd3NtqXLezRmMwSkN34e0MgzsOBn53rk/YWmCbgDOTrEhyAvD0OSrjh4CfSfITbaDMa5j+d//9wCsZguG8ceW4Bdia5FjgZSOW4YPA6Uke1oJ5fPkPZGiN3dHu1z2vW3cDQxfmAyc59t8BxyR5XpJlSX4ReBjw0RHLNr4cE17nqrqe4d7fn7dBOsuTjAXnXwIvSvKUJPsluX+7PgBfBJ7btt8IPGeEMtzJ0KpfzdAqHyvDDoZu6TcmuV9rXZ7QWve0INwBvAFbi7vNYJSG+1mrGP4b/yxw0Tyd9/kMA1huYrivdy7DH8SJ7HYZq+pK4H8whN31DPehvjnNbh9gGBDyiaq6sVv+GwyhdSvwzlbmUcrwsVaHTwBXta+9lwOvSXIrwz3RD3b7bgP+APh0htGwjxt37JuAn2Fo7d3EMBjlZ8aVe1TTXecXAncxtJq/y3CPlaq6jGFwz1nAFuBT3NOK/W2GFt7NwO+xcwt8Iu9haLF/C/hKK0fvN4AvA58Hvgf8MTv/LX8P8KMM96y1G3yDv7SXSHIu8NWqmvMWq/ZdSU4Dzqiqn1josixWthilBZLk0Uke1LreTma4r3T+dPtJk2nd1C8H3rHQZVnMDEZp4RzG8FaCrQzvwXtZVV2xoCXSopXkaQz3Y/+T6btrNQW7UiVJ6thilCSp44eI7wPWr19fGzZsWOhiSNKicvnll99YVYeOX24w7gM2bNjApk2bFroYkrSoJBn/iUmAXamSJO3EYJQkqWMwSpLUMRglSeoYjJIkdaYMxvZ8tKeNW/arSd46xT4Xt0+QJ8nfTfSIliRnJpnsCdpj2zyrPXl87PVrkox/6vduS/KmJN9qzy2TJAmYvsX4AeC545Y9ty2fVlWdUlXf352CAc9ieHTM2LF+p6r+cTePtZMWhs9meIbaE2fjmJOcx7fDSNIiM10wfgj46fYMN5JsYHha9j8leWuSTUmuTPJ7E+2c5Jok69v8q5P8W5JLGR4EOrbNLyf5fJLNSf6mPUH7ROAZwJ8m+WL7oOVzkjyn7fOUJFck+XKSs8eeRdbO93tJvtDWHTtBsQBOAq5keOjqqV1Z7pPkI60sm1s5SHJaki+1Ze9ty35YnvZ6a/t6UpJ/SnIBwyNjSHJ+ksvbtTqj2+fkVtbNST7ePkz635Mc2tbvl+SqsdeSpLk3ZTBW1feAyxie5A1Da/GDNXzA6quraiPwY8ATk/zYZMdJ8qi27yOBU4BHd6s/XFWPrqpHAP8KvKSq/pnhad6vqqpHVtXXu2OtBM4BfrGqfpThQwr6h6XeWFXHM4TeZN21pzK0ej/CEPzL2/I3A59qZTkeuDLJccBvAU9uy185WT07xwOvrKpj2usXV9WjgI3AK5Ic0sLuncDPteP+fHsI6fsYntMHwxO8N1fVDeNPkOSM9o/Jphtu2GW1JGk3jXJ/re9O7btRfyHJF4ArgOPouj0n8HjgI1W1rapuYQi9MQ9vLawvMwTCcdOU5yHA1VX1b+31uxmeMj7mw+3r5cCG8Tu31u8pwPmtLJ8Dxu6jPpkhUKmq7VW1pS07b+yhp+2fhelcVlVXd69fkWQzwwNHDwceDDwOuGRsu+64ZwOntfkXA++a6ARV9Y6q2lhVGw891AalJM2WUe6B/S1wVpLjgdVVdXmSoxhaY4+uqpuTnAOs3M0ynAM8q6o2JzmdoZtzT4w9AX07E9fvacBBwJeTAKwGbgc+OsPz3E37x6Lds1zRrbttbCbJSQwtvxOqaluSi5niWlXVdUn+M8mTgcdwT+tRkjQPpm0xVtVW4JMMLZmx1uJahj/+W5Lch3u6WidzCfCsJKuSHAg8vVt3IHB9687sQ+DWtm68rwEbkhzdXr8Q+NR09eicCry0qjZU1QbgKOAn2wM+P07rlk2yf5J1wCeAn09ySFt+cDvONcCj2vwzgOVMbB1wcwvFYxlaijC0Hp/Q/snojwvwFwxdqudV1fYZ1E2StIdGfavCB4BHtK9U1WaGLtSvMjwQ89NT7VxVXwDOBTYDHwM+363+bYbuzE+34435a+BVbZDNg7pj3QG8CDivdb/uAN42SiVa+J0MXNgd7zbgUoawfiXwpHbcy4GHVdWVwB8An2rdoW9su76T4d7qZuAEulbiOBcBy5L8K/A6hkCk3Tc8A/hwO8a53T4XAGuYpBtVkjR3fFDxXqi9D/Ssqnr8KNtv3LixfLqGJM1MksvbINKd+D67vUyS32TozvXeoiQtAD/1ZS9TVa+rqiOr6tKFLoskLUUGoyRJHYNRkqSOwShJUsdglCSpYzBKktQxGCVJ6hiMkiR1DEZJkjoGoyRJHYNRkqSOwShJUsdglCSpYzBKktQxGCVJ6hiMkiR1DEZJkjoGoyRJHYNRkqSOwShJUsdglCSpYzBKktQxGCVJ6hiMkiR1DEZJkjoGoyRJHYNRkqSOwShJUsdglCSps2yhCyBJEsD27XDHHTObXvQi2G+Wm3gGoyQJGILpzjsnD6Hbb595cM1kuvvumZf5ec+DVatm9zoYjJK0l9ixYwimuQ6gyaa77tqz8iewcuXO06pVO79eu3bXbfZkOuCA2bn2PYNRkpqxYFqIULrjDvjBD/a8DuODaPy0fv3sBlM/LV8+hONiZzBK2mtUzW8wjW+ZzUYwTRceBx+85wE0WfjtK8E0oR07YOtW2LLlnumWW+Dkk2e90gajpB+qGsJhPkJoounOO/e8DgccMHWoHHTQ6EEz02nFin04mPbE9u1DiPWhNj7gJls3tv6WW4Yf0PG2bZv1m4wGo7QXqRru88x1AE017akVK6YOnImCabamFStmf4TiknfXXdOH1nTrt26d/jzLl8O6dTtPRx893JQcv7yfli+f9SobjFKnahgZNx8BNNk00T/FM9EH00TT2rVw73vPTTAdcIDBtNcY65eeKrRGCbZR/ltauXLXwLr//Yev0wXb2PqVK/ea5rbBqL3O+GCa7xF6O3bsWfmXL586PNas2f0BENN1+RlM+4gquO220VpjU60fZZjpj/zIzkF18MFw1FG7BtdUobZixdxfk3lkMGoXo7aY5mravn3Pyr9s2dThsXr1zgMgZuv+0lgw7b//7HwftEjt2AG33rpnoXbLLdP/IiRw4IE7B9Vhh8FDHjJaqK1bN+y/zBgYzyuyhD396fDVr+7aOtvTYNp//6nDZtUquNe95q4rz99z7ba7774nsHY32G69dfr+8P333zW4jjhi8hbZRNOaNXYPzBH/hCxhxxxzT9f+bE4GkxbED34wswEhEy2/7bbpz7Nixa7BdfTRU3c1jp9Wr95r7qdpV/4JW8Le8IaFLoHE0Lq6447RQ22ybUYZJLJq1a7BNTZIZNRgW7ly7q+JFpTBKGn3jQ0Smenw/d0ZJLJmzc4BdcghOw8SmS7Y1q7d5waJaG4YjNJSNTZIZCahNn6bUQeJjG953fe+cOyxow/lX7vWUU2aNwajtBj1g0R2t7U2k0Ei/XTkkaOPely71kEiWnQMRmm+9YNEdre1NtNBImPTfe4zs1BzkIiWIINRGtX4QSK721obZZDI6tW7Btfhh48+lH/durl5Ho+0BBiMWhomGiSyO6E2yiCRsTddjwXX+vXwoAeNPpR/7do5+fxHSaMxGLX3m2iQyO50QU73WW/77bfrKMb73Q8e+tDRh/IfeKCDRKRFzmDU3JpskMhMgu3WW6c/z7Jlu4bUhg2jD+Uf+yQR76dJS57BqMndeefuv9l6bNq2bfrzHHDArqF12GGjD+Vft25447ahJmkWGIxL2WtfC9/4xuShNspTY1ev3jWwjjhi+lDrg81BIpL2IgbjUnbRRXDttfcE1aGH3vOZj6O01hwkImkfZDAuZZdeutAlkKS9jh9HIUlSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOgajJEkdg1GSpI7BKElSx2CUJKljMEqS1DEYJUnqGIySJHUMRkmSOrMSjEkOSfLFNn0nybe61yum2XdjkjePcI5/no2ydsd7Uyun/xxIkn5o2WwcpKpuAh4JkORMYGtVvX5sfZJlVXX3JPtuAjaNcI4TZ6OsrTz7Ac8GrgOeCHxyto497jyT1luStHeas9ZSknOSvC3J54A/SfKYJJ9JckWSf07ykLbdSUk+2ubPTHJ2kouTfCPJK7rjbe22vzjJh5J8NclfJUlbd0pbdnmSN48ddwInAVcCbwVO7c5xnyQfSbK5TSe25acl+VJb9t6ufs+ZpHz/lOQC4Ctt2fmtTFcmOaPb5+QkX2jH/XiS/ZL8e5JD2/r9klw19lqSNPdmpcU4hQcAJ1bV9iRrgcdX1d1Jngr8IfBzE+xzLPAk4EDga0neWlV3jdvmx4HjgG8Dnwb+S5JNwNuBJ1TV1Uk+MEW5TgU+APwt8IdJlrdzvBn4VFU9O8n+wJokxwG/1epxY5KDR6j38cDDq+rq9vrFVfW9JKuAzyf5G4Z/St7ZlffgqtqR5H3A84E3AU8FNlfVDeNP0AL2DIAjjjhihCJJkkYx1/fXzquq7W1+HXBekn8BzmIItolcWFV3VtWNwHeB+0ywzWVV9c2q2gF8EdjAEKjf6MJowmBs9zxPAc6vqluAzwFPa6ufzNCKpKq2V9WWtuy8Vh6q6nsj1PuyrhwAr0iyGfgscDjwYOBxwCVj23XHPRs4rc2/GHjXRCeoqndU1caq2njooTYoJWm2zHWL8bZu/veBT7bW2Abg4kn2ubOb387EZRxlm8k8DTgI+HLrgV0N3A5M1u06mbtp/1i0e5b9IKMf1jvJSQwtvxOqaluSi4GVkx20qq5L8p9Jngw8hqH1KEmaJ/M5InMd8K02f/ocHP9rwANb6AL84iTbnQq8tKo2VNUG4CjgJ5OsBj4OvAwgyf5J1gGfAH4+ySFt+VhX6jXAo9r8M4Dlk5xvHXBzC8VjGVqKMLQen5DkqHHHBfgL4H3s3OKWJM2D+QzGPwH+KMkVzEFLtapuB14OXJTkcuBWYEu/TQu/k4ELu/1uAy4Fng68EnhSki8DlwMPq6orgT8APtW6Q9/Ydn0n8MS27AR2bh33LgKWJflX4HUMgUi7b3gG8OF2jHO7fS4A1jBJN6okae6kqha6DLMmyZqq2tpGqb4F+PeqOmuhyzVTSTYCZ1XV40fZfuPGjbVp07TveJEkdZJcXlUbxy/f197c/stJvsjwVox1DKNUF5Ukvwn8DfB/F7oskrQU7VMtxqXKFqMkzdxSaTFKkrRHDEZJkjp2pe4DktwAXLubu68HbpzF4iwG1nlpWGp1Xmr1hT2v85FVtcsnpBiMS1ySTRP1se/LrPPSsNTqvNTqC3NXZ7tSJUnqGIySJHUMRr1joQuwAKzz0rDU6rzU6gtzVGfvMUqS1LHFKElSx2CUJKljMC4RSU5O8rUkV7XPYx2//oAk57b1n+se37UojVDf/5XkK0m+lOTjSY5ciHLOpunq3G33c0mqfVj9ojZKnZP8QvteX5nk/fNdxtk2ws/2EUk+meSK9vN9ykKUc7YkOTvJd9tD7idanyRvbtfjS0mO3+OTVpXTPj4B+wNfBx7I8EDlzQyP1Oq3eTnwtjb/XODchS73HNf3ScDqNv+yxVzfUevctjsQuITh8WcbF7rc8/B9fjBwBXCv9vreC13ueajzO4CXtfmHAdcsdLn3sM5PAI4H/mWS9acAHwPC8Lzbz+3pOW0xLg2PAa6qqm9U1Q+AvwaeOW6bZwLvbvMfAp7SHt+1GE1b36r6ZFVtay8/Czxgnss420b5HgP8PvDHwB3zWbg5Mkqdfxl4S1XdDFBV353nMs62UepcwNo2vw749jyWb9ZV1SXA96bY5JnAe2rwWeCgJPfdk3MajEvD/YHrutffbMsm3Kaq7mZ4yPMh81K62TdKfXsvYfiPczGbts6ti+nwqrqQfcMo3+djgGOSfDrJZ5OcPG+lmxuj1PlM4AVJvgn8HfAr81O0BTPT3/dpLduj4kiLXJIXABuBJy50WeZSkv2ANwKnL3BR5tsyhu7Ukxh6BS5J8qNV9f0FLdXcOhU4p6rekOQE4L1JHl5VOxa6YIuFLcal4VvA4d3rB7RlE26TZBlDF8xN81K62TdKfUnyVODVwDOq6s55Kttcma7OBwIPBy5Ocg3DvZgLFvkAnFG+z98ELqiqu6rqauDfGIJysRqlzi8BPghQVZ8BVjJ82Pa+aqTf95kwGJeGzwMPTnJUkhUMg2suGLfNBcAvtfnnAJ+odmd7EZq2vkl+HHg7Qygu9vtOME2dq2pLVa2vqg1VtYHhvuozqmoxP+F6lJ/r8xlaiyRZz9C1+o35LOQsG6XO/wE8BSDJQxmC8YZ5LeX8ugA4rY1OfRywpaqu35MD2pW6BFTV3Un+J/D3DKPazq6qK5O8BthUVRcAf8nQ5XIVw43u5y5ciffMiPX9U2ANcF4bY/QfVfWMBSv0HhqxzvuUEev898B/TfIVYDvwqqparD0ho9b514F3Jvk1hoE4py/if3JJ8gGGf27Wt/umvwssB6iqtzHcRz0FuArYBrxoj8+5iK+XJEmzzq5USZI6BqMkSR2DUZKkjsEoSVLHYJQkqWMwSpLUMRglSer8fyS8kLmK0/apAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9ckM7Qapdwk"
      },
      "source": [
        "## Week - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x7RtkFpmoG"
      },
      "source": [
        "### Cats & Dogs Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfeSKuNKpeK4"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYEbYA7Kpt3R"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9HctwL_p29E"
      },
      "source": [
        "The Training Accuracy is close to 100%, and the validation accuracy is in the 70%-80% range. This is a great example of overfitting -- which in short means that it can do very well with images it has seen before, but not so well with images it hasn't. Let's see if we can do better to avoid overfitting -- and one simple method is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too. \n",
        "\n",
        "What if we tweak with the images to change this up a bit -- rotate the image, squash it, etc.  That's what image augementation is all about. And there's an API that makes it easy...\n",
        "\n",
        "Now take a look at the ImageGenerator. There are properties on it that you can use to augment the image. \n",
        "\n",
        "```\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "```\n",
        "These are just a few of the options available (for more, see the Keras documentation. Let's quickly go over what we just wrote:\n",
        "\n",
        "* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
        "* width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
        "* shear_range is for randomly applying shearing transformations.\n",
        "* zoom_range is for randomly zooming inside pictures.\n",
        "* horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
        "* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
        "\n",
        "\n",
        "Here's some code where we've added Image Augmentation. Run it to see the impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wnWCxkp3LY"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIVCY_gp7oW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDNwJ6KTp9YC"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX3ekIi3qACk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ln0NbcjqI27"
      },
      "source": [
        "### Implementation on Horses & Humans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ThY0D-qPBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc90686-ae4a-4fd7-dbbd-442a7803de38"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()\n",
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 11:20:25--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.128, 172.217.204.128, 172.217.203.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M   113MB/s    in 1.3s    \n",
            "\n",
            "2021-07-22 11:20:26 (113 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2021-07-22 11:20:26--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.213.128, 173.194.214.128, 173.194.215.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.213.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "/tmp/validation-hor 100%[===================>]  10.95M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-07-22 11:20:27 (119 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RX4xUV1qXpT"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhjtlWHmrDoJ"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXzg4jEQrFs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743a09db-56a3-4dc7-a596-4c39698b90de"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4C5CABIrHAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afca59a-76d0-43a9-807f-813425061b53"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFHcFcCLrJjE"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KvFBRuarKlj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPujhQdymQW7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycvGz4acmRUG"
      },
      "source": [
        "## Week - 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLMLAqMBBm00"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQdYkbUumVE_"
      },
      "source": [
        "Get a specific layer from the DNN and customize (we will implement 'Dropout') it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snzvmzhnmRzh"
      },
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers: #it makes freeze (lock) model layer\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9TYp2opmxu6"
      },
      "source": [
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VJGSXxgmzav"
      },
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyfL0fX5qbYk"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKKqKMBHqq_n"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "       -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '//tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'train')\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') # Directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs') # Directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats') # Directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')# Directory with our validation dog pictures\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 20,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (150, 150))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 20,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMst7lMQqxQx"
      },
      "source": [
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            steps_per_epoch = 100,\n",
        "            epochs = 20,\n",
        "            validation_steps = 50,\n",
        "            verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W4Ez3MGqyRM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEOxxH41Bhem"
      },
      "source": [
        "### Lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfEgtemABiN6"
      },
      "source": [
        "# ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n",
        "# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n",
        "# ATTENTION: Please use the provided epoch values when training.\n",
        "\n",
        "# Import all the necessary files!\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from os import getcwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju35cNiXBvRv"
      },
      "source": [
        "path_inception = f\"{getcwd()}/../tmp2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = path_inception\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150,150,3),\n",
        "                               include_top = False,\n",
        "                               weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# Make all the layers in the pre-trained model non-trainable\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# Print the model summary\n",
        "#pre_trained_model.summary()\n",
        "\n",
        "# Expected Output is extremely large, but should end with:\n",
        "\n",
        "#batch_normalization_v1_281 (Bat (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n",
        "#__________________________________________________________________________________________________\n",
        "#activation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_v1_273[0][0] \n",
        "#__________________________________________________________________________________________________\n",
        "#mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n",
        "#                                                                 activation_276[0][0]             \n",
        "#__________________________________________________________________________________________________\n",
        "#concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n",
        "#                                                                 activation_280[0][0]             \n",
        "#__________________________________________________________________________________________________\n",
        "#activation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_v1_281[0][0] \n",
        "#__________________________________________________________________________________________________\n",
        "#mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n",
        "#                                                                 mixed9_1[0][0]                   \n",
        "#                                                                 concatenate_5[0][0]              \n",
        "#                                                                 activation_281[0][0]             \n",
        "#==================================================================================================\n",
        "#Total params: 21,802,784\n",
        "#Trainable params: 0\n",
        "#Non-trainable params: 21,802,784"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjNi3zJHBvrw"
      },
      "source": [
        "last_layer = pre_trained_model.get_layer(\"mixed7\")\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "# Expected Output:\n",
        "# ('last layer output shape: ', (None, 7, 7, 768))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JSbzy2eByb8"
      },
      "source": [
        "# Define a Callback class that stops training once accuracy reaches 97.0%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc') is not None and logs.get('acc') >= 0.998):\n",
        "      print(\"\\nReached 97.0% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "  \n",
        "  on_epoch_end()\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K9-v_StBzfp"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024,activation = \"relu\")(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1,activation = \"sigmoid\")(x)           \n",
        "\n",
        "model = Model(pre_trained_model.input,x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = \"binary_crossentropy\", \n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "# Expected output will be large. Last few lines should be:\n",
        "\n",
        "# mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_248[0][0]             \n",
        "#                                                                  activation_251[0][0]             \n",
        "#                                                                  activation_256[0][0]             \n",
        "#                                                                  activation_257[0][0]             \n",
        "# __________________________________________________________________________________________________\n",
        "# flatten_4 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n",
        "# __________________________________________________________________________________________________\n",
        "# dense_8 (Dense)                 (None, 1024)         38536192    flatten_4[0][0]                  \n",
        "# __________________________________________________________________________________________________\n",
        "# dropout_4 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
        "# __________________________________________________________________________________________________\n",
        "# dense_9 (Dense)                 (None, 1)            1025        dropout_4[0][0]                  \n",
        "# ==================================================================================================\n",
        "# Total params: 47,512,481\n",
        "# Trainable params: 38,537,217\n",
        "# Non-trainable params: 8,975,264\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKNX1udTB0gI"
      },
      "source": [
        "# Get the Horse or Human dataset\n",
        "path_horse_or_human = f\"{getcwd()}/../tmp2/horse-or-human.zip\"\n",
        "# Get the Horse or Human Validation dataset\n",
        "path_validation_horse_or_human = f\"{getcwd()}/../tmp2/validation-horse-or-human.zip\"\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/tmp')\n",
        "local_zip = path_horse_or_human\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = path_validation_horse_or_human\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUEHBzxRB2ae"
      },
      "source": [
        "# Define our example directories and files\n",
        "train_dir = '/tmp/training'\n",
        "validation_dir = '/tmp/validation'\n",
        "\n",
        "train_horses_dir = os.path.join(train_dir, 'horses')\n",
        "train_humans_dir = os.path.join(train_dir, 'humans')\n",
        "validation_horses_dir = os.path.join(validation_dir, 'horses')\n",
        "validation_humans_dir = os.path.join(validation_dir, 'humans')\n",
        "\n",
        "train_horses_fnames = os.listdir(train_horses_dir)\n",
        "train_humans_fnames = os.listdir(train_humans_dir)\n",
        "validation_horses_fnames = os.listdir(validation_horses_dir)\n",
        "validation_humans_fnames = os.listdir(validation_humans_dir)\n",
        "\n",
        "print(len(train_horses_fnames))\n",
        "print(len(train_humans_fnames))\n",
        "print(len(validation_horses_fnames))\n",
        "print(len(validation_humans_fnames))\n",
        "\n",
        "# Expected Output:\n",
        "# 500\n",
        "# 527\n",
        "# 128\n",
        "# 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVxOqwAPB3o6"
      },
      "source": [
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255,\n",
        "                                  rotation_range = 45,\n",
        "                                  width_shift_range = 0.2,\n",
        "                                  height_shift_range = 0.2,\n",
        "                                  shear_range = 0.2,\n",
        "                                  zoom_range = 0.2,\n",
        "                                  horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented! - > OK!\n",
        "test_datagen = ImageDataGenerator(rescale = 1/255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                   batch_size=20,\n",
        "                                                   class_mode = \"binary\",\n",
        "                                                   target_size = (150,150))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                   batch_size=20,\n",
        "                                                   class_mode = \"binary\",\n",
        "                                                   target_size = (150,150))\n",
        "\n",
        "# Expected Output:\n",
        "# Found 1027 images belonging to 2 classes.\n",
        "# Found 256 images belonging to 2 classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDPEEmpjB4zC"
      },
      "source": [
        "# Run this and see how many epochs it should take before the callback\n",
        "# fires, and stops training at 97% accuracy\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            steps_per_epoch = 20,\n",
        "            epochs = 3,\n",
        "            validation_steps = 10,\n",
        "            verbose = 2,\n",
        "            callbacks = [callbacks])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jkbD4dhB5sR"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2dvu0Rh8foY"
      },
      "source": [
        "## Week - 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5xqtZ38o2c"
      },
      "source": [
        "### Multi-class Classification: Rock Paper Scissors Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A5pTj798goq"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQYCI_x3862K"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3emHbdP877z"
      },
      "source": [
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2O_Nlp872Q"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_rock = [os.path.join(rock_dir, fname) \n",
        "                for fname in rock_files[pic_index-2:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname) \n",
        "                for fname in paper_files[pic_index-2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) \n",
        "                for fname in scissors_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
        "  #print(img_path)\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlB19ELi8--2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"/tmp/rps/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/rps-test-set/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)\n",
        "\n",
        "model.save(\"rps.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3XPBjFR9BDo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc1o726S9CIV"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}